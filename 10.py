# -*- coding: utf-8 -*-
"""10_EDA_ДЗ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hcTOKt2znGCmAqebEtW9jb_qseNugKLt

#Pipeline описательного анализа данных
"""

# Commented out IPython magic to ensure Python compatibility.
# импортируем библиотеки
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pandas import DataFrame
from google.colab import files

# %matplotlib inline

sns.set_style('darkgrid')

"""### Задача 1

Для датасета Ирисы, который мы разбирали поэтапно на протяжении всего курса, сделайте описательный анализ данных (EDA - Exploratory Data Analysis) по шагам (по сути, вам нужно из всего курса собрать нужные куски кода, т.к. все это уже так или иначе было сделано для этого датасета):

* подгрузите данные
* посмотрите какого размера данные, сколько наблюдений, сколько признаков, сколько классов имеет целевая переменная
* проверьте данные на пропуски
* постройте графики попарных зависимостей между признаками и раскрасьте цветом данные по классам
* постройте боксплоты/violinplots для признаков
* постройте боксплоты/violinplots для признаков с разметкой по классам
* постройте любой график/графики, который по вашему мнению добавляет информации о существующих взаимосвязях в данных
* сделайте вывод
"""

# Ирисы живут в sklearn
import sklearn
from sklearn import datasets
# вытащите таргет и признаки из словаря ниже самостоятельно и сформируйте pd.DataFrame 
# c красивыми и правильными именами столбцов
data = datasets.load_iris()

data.keys()

print (data.DESCR)

data.data[:10]

data.target_names

data.target

iris = DataFrame(data.data)
iris.columns = data.feature_names
iris['target']=data.target

iris.describe()

iris.info()

iris.isnull().sum()

iris.head()

iris.target = iris.target.apply(lambda x : data.target_names[x])

iris.head()

iris.columns.tolist()

iris.shape

np.round(iris.isna().sum()[iris.isna().sum() > 0] / iris.shape[0], 2)

plt.subplots(figsize = (16,20))

plot_number = 0
for feature_name in data['feature_names']:
  for target_name in data['target_names']:
    plot_number += 1
    pyplot.subplot(4,3, plot_number)
    pyplot.hist(iris[iris.target == target_name][feature_name])
    pyplot.title(target_name)
    pyplot.xlabel('cm')
    pyplot.ylabel(feature_name[:-4])

# смотрим график в пар-признаках
sns.pairplot(iris, hue='target')

korr = iris.corr()
korr

plt.figure(figsize=(10,11))
sns.heatmap(iris.corr(),annot=True)
plt.plot()

plt.subplots(figsize = (16,20))

plot_number = 0
for feature_name in data['feature_names']:
  for target_name in data['target_names']:
    plot_number += 1
    pyplot.subplot(4,3, plot_number)
    sns.violinplot(iris[iris.target == target_name][feature_name])
    pyplot.title(target_name)
    pyplot.xlabel('cm')
    pyplot.ylabel(feature_name[:-4])

plt.subplots(figsize = (16,20))

plot_number = 0
for feature_name in data['feature_names']:
  for target_name in data['target_names']:
    plot_number += 1
    pyplot.subplot(4,3, plot_number)
    sns.boxplot(iris[iris.target == target_name][feature_name])
    pyplot.title(target_name)
    pyplot.xlabel('cm')
    pyplot.ylabel(feature_name[:-4])

import numpy as np
iris_setosa = iris.loc[iris['target'] == 'setosa']
iris_virginica = iris.loc[iris['target'] == 'virginica']
iris_versicolor = iris.loc[iris['target'] == 'versicolor']

plt.plot(iris_setosa['petal length (cm)'], np.zeros_like(iris_setosa['petal length (cm)']), 'o')
plt.plot(iris_versicolor['petal length (cm)'], np.zeros_like(iris_versicolor['petal length (cm)']), 'o')
plt.plot(iris_virginica['petal length (cm)'], np.zeros_like(iris_virginica['petal length (cm)']), 'o')

"""# Как видно на графике, синие точки - это сетоза, оранжевые - разноцветные, а зеленые - виргинские. Здесь построино каждое наблюдение, и одну вещь, которую можно заметить, - это то, что 1-мерный точечный график очень очень трудно прочитать и понять,

#Среднее, дисперсия и стандартное отклонение
"""

print('Среднее')
print(np.mean(iris_setosa['petal length (cm)']))
print ('С выбросом')
print(np.mean(np.append(iris_setosa['petal length (cm)'],50)))
print(np.mean(iris_virginica['petal length (cm)']))
print(np.mean(iris_versicolor['petal length (cm)']))
print('Std-dev:')
print(np.std(iris_setosa['petal length (cm)']))
print(np.std(iris_virginica['petal length (cm)']))
print(np.std(iris_versicolor['petal length (cm)']))

"""# Глядя на среднее значение, можно сказать, что сатос имеет меньшую длину petal length по сравнению с вирджиникой и разноцветой. И вирджиника и разноцветка имеют немного меньшую длину petal."""

sns.pairplot(iris, hue='target')

"""# 1. petal_length и petal_width являются наиболее полезными функциями для идентификации различных типов цветов.
# 2. в то время как Setosa может быть легко идентифицирована (линейно разделима), virginica и versicolor имеют некоторое совпадение.
# 3. можно найти условия "линий" и "условий", чтобы построить простую модель для классификации типов цветов.

### Задача 2

Для датасета House Prices из последней лекции (train.csv) постройте такие графики взаимосвязей, которые посчитаете нужными, для того, чтобы добавить информации к тому, что мы уже сделали на лекции. Пара заданий уже есть в тексте лекции. Сделайте вывод.
"""

file = files.upload()

data = pd.read_csv('train (1).csv')
data.head(5)

k = 8 
corrmat = data.corr()
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index.tolist()
cols

price_bins=data.SalePrice.quantile([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])
price_bins

cm = np.corrcoef(data[cols].values.T)
plt.figure(figsize=(10,7))
sns.set(font_scale=1.25)
sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12},\
                yticklabels=cols, xticklabels=cols, vmin=-1, center=0,\
                    cmap=sns.color_palette('coolwarm',1000))

sns.lmplot(data=data, x='LotFrontage', y='SalePrice', aspect=3, height=5)
plt.show()

plt.figure(figsize=(10,6))
sns.boxplot(data=data, x='SalePrice', y='MSZoning', hue='MSZoning')
plt.show()

"""Нмного приобразуем данные SalePrice сделав log"""

data['SalePrice_Log'] = np.log(data['SalePrice'])
sns.distplot(data['SalePrice_Log'])

"""графики парных зависимостей для топ 8 признаков 'SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', '1stFlrSF'"""

col = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', '1stFlrSF']

sns.pairplot(data[col])

data['PriceGroups'] = pd.cut(data['SalePrice'], price_bins, \
                             labels=['0-0.1', '0.1-0.2', '0.2-0.3','0.3-0.4','0.4-0.5',\
                                     '0.5-0.6', '0.6-0.7', '0.7-0.8',\
                                     '0.8-0.9', '0.9-1'], right=True, include_lowest=True)

plt.figure(figsize=(15,8))
sns.boxplot(data=data, y='GrLivArea', x='PriceGroups', hue='PriceGroups')

plt.figure(figsize=(15,8))
sns.boxplot(data=data, y='OverallQual', x='PriceGroups', hue='PriceGroups')

plt.figure(figsize=(10,6))
sns.boxplot(data=data, x='SalePrice', y='MSZoning', hue='MSZoning')
plt.show()

plt.figure(figsize=(10,6))
sns.boxplot(data=data, x='TotalBsmtSF', y='MSZoning', hue='MSZoning')
plt.show()

plt.figure(figsize=(10,6))
sns.boxplot(data=data, x='TotRmsAbvGrd', y='MSZoning', hue='MSZoning')
plt.show()

plt.figure(figsize=(10,6))
sns.boxplot(data=data, x='FullBath', y='MSZoning', hue='MSZoning')
plt.show()

plt.figure(figsize=(20, 5))

features = ['OverallQual', 'GrLivArea']
target = data['SalePrice']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = data[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('SalePrice')

from scipy.stats import norm

plt.figure(figsize=(10,7))
sns.distplot(data.SalePrice, fit=norm)

# фитим распределение
(mu, sigma) = norm.fit(data.SalePrice)

plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice Density Distribution')

plt.figure(figsize=(10,7))
plt.show()

"""# Цены растут по мере линейного увеличения  'OverallQual', что и логично. Цены имеют тенденцию к снижению при уменьшении 'GrLivArea'

### Задача 1

Возьмите любой датасет **для задачи регрессии** (с работы, с учебы, из репозиториев) и проведите описательный анализ данных. Сделайте краткий вывод о найденных взаимосвязях.
"""

from sklearn.feature_selection import RFE
from sklearn.ensemble import ExtraTreesRegressor
from scipy import stats
from scipy.stats import norm

file = files.upload()

dataframe = pd.read_csv('forestfires.csv')

"""Информация об атрибутах:

Для получения дополнительной информации читайте [Cortez and Morais, 2007]. 
1. X - пространственная координата оси X на карте парка Монтезиньо: от 1 до 9 
2. Y - пространственная координата оси Y на карте парка Монтесиньо: от 2 до 9 
3. месяц - месяц года: от «января» до « dec ' 
4. день - день недели: с 
понедельника по воскресенье 5. FFMC - индекс FFMC из системы FWI: с 18,7 до 96,20 
6. DMC - индекс DMC из системы FWI: с 1,1 до 291,3 
7. DC - Индекс постоянного тока из системы FWI: от 7,9 до 860,6. 
8. ISI - индекс ISI из системы FWI: от 0,0 до 56,10. 
9. temp - температура в градусах Цельсия: от 2,2 до 33,30. 
10. RH - относительная влажность в%: от 15,0 до 100. 
11. ветер - скорость ветра в км / ч: от 0,40 до 9,40 
12. дождь - наружный дождь в мм / м2: от 0,0 до 6,4; 
13. площадь - площадь сожженного леса (в га): от 0,00 до 1090,84
"""

dataframe.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)
dataframe.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace=True)

print('содержание')
dataframe.head()

print('статистика')
dataframe.describe()

print('типы')
dataframe.dtypes

dataframe.isna().sum()

dataframe.isnull().sum()

dataframe.shape

plt.subplots(figsize=(14,11))
sns.heatmap(dataframe.corr(), cmap=sns.color_palette("coolwarm", 100), vmin=-1, center=0)
plt.show()

plt.hist((dataframe.area))

"""Большая часть выборок набора данных находится в диапазоне от 0 до 200 выходного класса 'Area', при этом большинство составляет менее 100"""

plt.figure(figsize=(10,7))
sns.distplot(dataframe.area, fit=norm)

# фитим распределение
(mu, sigma) = norm.fit(dataframe.area)

plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')
plt.title('Area of spread, burnt forest')

plt.figure(figsize=(10,7))
stats.probplot(dataframe['area'], plot=plt)
plt.show()

col = dataframe.columns.tolist()
col

dataframe.quantile(0.99)

dataframe['area'] = 'area'

plt.figure(figsize=(0.5,1))
sns.pairplot(dataframe[col], hue='area')
plt.title('Визуализация совмесных распределений')

"""Среди других атрибутов есть смесь положительных перекосов и негативных перекосов."""

col

dataframe['Total_S'] = dataframe.X * dataframe.Y

sns.lmplot(data=dataframe, y='Total_S', x='month')
plt.title('Зависимость координат Х и У от месяца')

sns.lmplot(data=dataframe, y='Total_S', x='day')
plt.title('Зависимость координат Х и У от дня недели')

sns.lmplot(data=dataframe, y='Total_S', x='temp')
plt.title('Зависимость координат Х и У от температуры')

sns.lmplot(data=dataframe, y='month', x='temp')
plt.title('Зависимость температуры от месяца')
plt.show()

"""Существует прямая зависимость пожаров от месяца, дня недели и температу, что и логично

### Задача 2

Возьмите любой датасет **для задачи классификации** (с работы, с учебы, из репозиториев) и проведите описательный анализ данных. Сделайте краткий вывод о найденных взаимосвязях.
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from pandas import DataFrame
from google.colab import files

# %matplotlib inline

# классификация
# датасет про рак молочной железы
data = datasets.load_breast_cancer()
print("Features: ", data.feature_names)
print("Labels: ", data.target_names)
print(data.data.shape)
print(data.data[10:14,:])
print(data.target)
target = data.target
#меняем 0 и 1, чтобы злокачественная была 1
target = 1-target

data.keys()

print (data.DESCR)

data.data[:10]

data.target_names

data.target

rak = DataFrame(data.data)
rak.columns = data.feature_names
rak['target']=data.target

rak.describe()

rak.info()

rak.head()

rak.target = rak.target.apply(lambda x : data.target_names[x])
rak.head()

rak.columns.tolist()

rak.isna().sum()

rak.shape

# смотрим график в пар-признаках
sns.pairplot(rak, hue='target')

korr = rak.corr()
korr

plt.figure(figsize=(10,11))
sns.heatmap(rak.corr(),annot=True)
plt.plot()

"""Выбрать какой один или пару признаков по которым можно было бы как-то однозначно классифицировать есть рак молочный железы или нет, не представляется возможным, так все перечисленные признаки влияют на вывод (ошибку) есть или нет рак молочный железы

## Полезные ссылки 

Ссылки на ресурсы, где можно взять датасет для анализа

https://www.kaggle.com/datasets - Kaggle, платформа для соревнований по машинному обучению. Кроме самих соревнований - очень много полезной информации по анализу данных и машинному обучению с примерами кода.


https://archive.ics.uci.edu/ml/index.php - репозиторий с классическими датасетами, по ним пишут статьи, на них сравнивают разные алгоритмы машинного обучения
"""